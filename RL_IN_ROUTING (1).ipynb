{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import heapq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# A* algorithm to find the shortest path between two points\n",
        "def astar(maze, start, goal):\n",
        "    def heuristic(a, b):\n",
        "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
        "\n",
        "    def get_neighbors(x, y):\n",
        "        neighbors = []\n",
        "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "        for dx, dy in directions:\n",
        "            nx, ny = x + dx, y + dy\n",
        "            if 0 <= nx < len(maze) and 0 <= ny < len(maze[0]) and maze[nx][ny] != 1:\n",
        "                neighbors.append((nx, ny))\n",
        "\n",
        "        return neighbors\n",
        "\n",
        "    open_list = []\n",
        "    heapq.heappush(open_list, (0 + heuristic(start, goal), 0, start))\n",
        "    came_from = {}\n",
        "    g_score = {start: 0}\n",
        "    f_score = {start: heuristic(start, goal)}\n",
        "\n",
        "    while open_list:\n",
        "        _, g, current = heapq.heappop(open_list)\n",
        "\n",
        "        if current == goal:\n",
        "            path = []\n",
        "            while current in came_from:\n",
        "                path.append(current)\n",
        "                current = came_from[current]\n",
        "            path.append(start)\n",
        "            path.reverse()\n",
        "            return path\n",
        "\n",
        "        for neighbor in get_neighbors(*current):\n",
        "            tentative_g_score = g + 1\n",
        "            if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n",
        "                came_from[neighbor] = current\n",
        "                g_score[neighbor] = tentative_g_score\n",
        "                f_score[neighbor] = g_score[neighbor] + heuristic(neighbor, goal)\n",
        "                heapq.heappush(open_list, (f_score[neighbor], g_score[neighbor], neighbor))\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nawILJHc-VB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment creation consisting of pins, ed_point, start_point\n",
        "def create_environment(n, num_pins):\n",
        "    if n <= 1:\n",
        "        raise ValueError(\"Maze size must be greater than 1.\")\n",
        "    if num_pins >= n * n:\n",
        "        raise ValueError(\"Too many pins for the maze size. Reduce the number of pins.\")\n",
        "\n",
        "    maze = np.zeros((n, n), dtype=int)\n",
        "    start_point = (random.randint(0, n - 1), random.randint(0, n - 1))\n",
        "    while True:\n",
        "        end_point = (random.randint(0, n - 1), random.randint(0, n - 1))\n",
        "        if end_point != start_point:\n",
        "            break\n",
        "    maze[start_point[0]][start_point[1]] = 2\n",
        "    maze[end_point[0]][end_point[1]] = 2\n",
        "\n",
        "    pins = []\n",
        "    #pins.append(start_point)\n",
        "    while len(pins) < num_pins:\n",
        "        pin_location = (random.randint(0, n - 1), random.randint(0, n - 1))\n",
        "        if maze[pin_location[0]][pin_location[1]] == 0:\n",
        "            maze[pin_location[0]][pin_location[1]] = 1\n",
        "            pins.append(pin_location)\n",
        "\n",
        "    # Visualize\n",
        "    '''\n",
        "    plt.imshow(maze, cmap='viridis', origin='upper')\n",
        "    plt.colorbar(label='Legend (0=Empty, 1=Pin, 2=Start, 3=End)')\n",
        "    plt.grid(visible=True, color='white', linestyle='-', linewidth=0.5)\n",
        "    plt.title(\"Maze with Start, End, and Pins\")\n",
        "    plt.show()'''\n",
        "\n",
        "    return maze, start_point, end_point, pins"
      ],
      "metadata": {
        "id": "6HSlAVO-R4c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep Q network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dims, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.state_dims = state_dims\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        self.q_network = nn.Sequential(\n",
        "            nn.Linear(self.state_dims, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.num_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.q_network(state)\n"
      ],
      "metadata": {
        "id": "lFud4pV-R456"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MazeEnvironment:\n",
        "    def __init__(self, maze_size, num_pins):\n",
        "        self.maze, self.start_point, self.end_point, self.pins = create_environment(maze_size, num_pins)\n",
        "        self.state_dims = maze_size * maze_size\n",
        "        self.num_actions = len(self.pins)\n",
        "        self.state = self.start_point\n",
        "        self.agent_path = []\n",
        "\n",
        "    def reset(self):\n",
        "        self.maze, self.start_point, self.end_point, self.pins = create_environment(len(self.maze),num_actions)\n",
        "        self.state = self.start_point\n",
        "        self.agent_path = []\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action,epsilon):\n",
        "\n",
        "        if len(self.pins) == 0:\n",
        "            raise ValueError(\"No pins left to select action.\")\n",
        "        if action < 0 or action >= len(self.pins):\n",
        "            action = random.randint(0, len(self.pins) - 1)\n",
        "            #raise ValueError(f\"Action {action} is out of bounds. Valid range: 0 to {len(self.pins)-1}\")\n",
        "        #print(f\"before action : {action} pins left are {len(self.pins)}\")\n",
        "\n",
        "        pair = self.pins[action]\n",
        "        if env.maze[self.start_point[0]][self.start_point[1]] == 1 or env.maze[self.end_point[0]][self.end_point[1]] == 1:\n",
        "            #print(f\"goal is blocked! Goal: {self.end_point}\")\n",
        "            env.maze[self.start_point[0]][self.start_point[1]] == 2\n",
        "            env.maze[self.end_point[0]][self.end_point[1]] == 2\n",
        "        path = astar(self.maze, pair, self.end_point)\n",
        "\n",
        "        #print(path)\n",
        "        if path:\n",
        "\n",
        "            for (x,y) in path:\n",
        "              env.maze[x][y]=1      #dynamic route blockages\n",
        "\n",
        "              reward = 1 - 0.01 * len(path) #accounting for the path length instead of only successful connection\n",
        "        else:\n",
        "              reward=-0.1\n",
        "        #print(\"Maze after routing pin:\", action)\n",
        "        '''\n",
        "        for row in env.maze:\n",
        "            print(row)'''\n",
        "        self.pins.remove(pair)\n",
        "        #print(f\"after action : {action} pins left are {len(self.pins)}\")\n",
        "        done = len(self.pins) == 0\n",
        "        self.agent_path.extend(path)\n",
        "        return self._get_state(), reward, done\n",
        "\n",
        "    def _get_state(self):\n",
        "        state = np.zeros_like(self.maze)\n",
        "        state[self.state[0]][self.state[1]] = 1\n",
        "        return state.flatten()\n",
        "\n",
        "def visualize_routing_after_episode(maze, start, end, agent_path, pins):\n",
        "    maze_copy = maze.copy()\n",
        "    maze_copy[start[0], start[1]] = 2\n",
        "    maze_copy[end[0], end[1]] = 3\n",
        "    for pin in pins:\n",
        "        maze_copy[pin[0], pin[1]] = 4\n",
        "\n",
        "    plt.imshow(maze_copy)\n",
        "    #plt.colorbar(label='Legend (0=Empty, 2=Start, 3=End, 4=Pin)')\n",
        "    plt.title(\"Maze After Episode\")\n",
        "    plt.show()\n",
        "\n",
        "    for (x, y) in agent_path:\n",
        "        maze_copy[x, y] = 5\n",
        "    plt.imshow(maze_copy)\n",
        "    #plt.colorbar(label='Legend (0=Empty, 2=Start, 3=End, 4=Pin, 5=Path)')\n",
        "    plt.title(\"Maze with Routing Path After Episode\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "49ymPrUQR5GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(agent, env, num_episodes=50, gamma=0.9, batch_size=30, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=0.001)\n",
        "    replay_buffer = deque(maxlen=3000)\n",
        "\n",
        "    episode_rewards = []\n",
        "    losses = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        #print(\"new Episode :+++++++++++++++++++++++++\")\n",
        "        env = MazeEnvironment(env.maze.shape[0], len(env.pins))\n",
        "        state = env.reset()\n",
        "\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if len(env.pins) == 0:\n",
        "                print(f\"Episode {episode}: No pins left, ending episode.\")\n",
        "                done = True\n",
        "                break\n",
        "            if random.random() < epsilon:\n",
        "\n",
        "                action = random.randint(0, len(env.pins) - 1)\n",
        "                #print(f\"action selected {action}\")\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    q_values = agent(state)\n",
        "                    action = torch.argmax(q_values[:, :len(env.pins)]).item()\n",
        "\n",
        "            next_state, reward, done = env.step(action,epsilon)\n",
        "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "            total_reward += reward\n",
        "\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                batch = random.sample(replay_buffer, batch_size)\n",
        "                for s, a, r, next_s, d in batch:\n",
        "                    q_values = agent(s)\n",
        "                    next_q_values = agent(next_s)\n",
        "                    target = r + gamma * torch.max(next_q_values) * (1 - d)\n",
        "                    loss = nn.MSELoss()(q_values[0][a], target)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        if done:\n",
        "            print(f\"Episode {episode} done! Visualizing the path...\")\n",
        "            visualize_routing_after_episode(env.maze, env.start_point, env.end_point, env.agent_path, env.pins)\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "        #if episode%5==0:\n",
        "        print(f\"Episode {episode}/{num_episodes} - Reward: {total_reward}\")\n",
        "\n",
        "    return episode_rewards, losses\n",
        "\n",
        "\n",
        "def visualize_training(episode_rewards, losses):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(losses, label='Loss per Episode')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss per Episode during Training')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(episode_rewards, label='rewards')\n",
        "    plt.title('rewards ')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    rewards_smoothed = pd.Series(episode_rewards).rolling(window=10).mean()\n",
        "    plt.plot(rewards_smoothed, label=\"Smoothed Rewards\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "um5gGN_pR5Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maze_size = 100\n",
        "num_pins = 25\n",
        "env = MazeEnvironment(maze_size, num_pins)\n",
        "\n",
        "state_dims = env.state_dims\n",
        "num_actions = env.num_actions\n",
        "agent = DQN(state_dims, num_actions)\n",
        "\n",
        "episode_rewards, losses = train(agent, env)\n",
        "visualize_training(episode_rewards, losses)"
      ],
      "metadata": {
        "id": "YdC2TmvGSW_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}